{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# stellargraph\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "\n",
    "# sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "#tensorflow\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrimos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StellarGraph: Undirected multigraph\n",
      " Nodes: 25, Edges: 52\n",
      "\n",
      " Node types:\n",
      "  default: [25]\n",
      "    Features: none\n",
      "    Edge types: default-default->default\n",
      "\n",
      " Edge types:\n",
      "    default-default->default: [52]\n",
      "        Weights: range=[0.659464, 0.961213], mean=0.76453, std=0.0779707\n",
      "        Features: none\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#abrimos los datos\n",
    "with open(\"./Datos/graphs.txt\", \"rb\") as fp:   # Unpickling ../input/egg-new/26-0.2/graphs.txt\n",
    "    graphs = pickle.load(fp)\n",
    "\n",
    "with open(\"./Datos/labels.txt\", \"rb\") as fp:   # Unpickling\n",
    "    labels = pickle.load(fp)\n",
    "\n",
    "# visualizamos como estan los datos importados\n",
    "print(graphs[0].info())\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataframe de los grafos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nodes</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>906.0</td>\n",
       "      <td>906.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.0</td>\n",
       "      <td>52.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.0</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nodes  edges\n",
       "count  906.0  906.0\n",
       "mean    25.0   52.6\n",
       "std      0.0   35.4\n",
       "min     25.0    1.0\n",
       "25%     25.0   25.0\n",
       "50%     25.0   47.0\n",
       "75%     25.0   73.0\n",
       "max     25.0  223.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(\n",
    "    [(g.number_of_nodes(), g.number_of_edges()) for g in graphs],\n",
    "    columns=[\"nodes\", \"edges\"],\n",
    ")\n",
    "summary.describe().round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de las labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_labels = pd.get_dummies(labels, drop_first=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir en los splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 91\n",
      "Validation size: 123\n",
      "Training size: 692\n"
     ]
    }
   ],
   "source": [
    "training_graphs, pred_graphs = model_selection.train_test_split(\n",
    "    graph_labels, train_size=0.90, test_size=None, stratify=graph_labels, random_state=6,\n",
    ")\n",
    "\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    training_graphs, train_size=0.85, test_size=None, stratify=training_graphs, random_state=6,\n",
    "    \n",
    ")\n",
    "\n",
    "print('Test size:', len(pred_graphs.values))\n",
    "print('Validation size:', len(test_graphs.values))\n",
    "print('Training size:',len(train_graphs.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch con los grafos y las labels de cada set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = PaddedGraphGenerator(graphs=graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    batch_size=25,\n",
    "    symmetric_normalization=True,\n",
    "    weighted = True\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    batch_size=25,\n",
    "    symmetric_normalization=True,\n",
    "    weighted = True\n",
    ")\n",
    "\n",
    "pred_gen = gen.flow(\n",
    "    list(pred_graphs.index - 1),\n",
    "    targets=pred_graphs.values,\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=True,\n",
    "    weighted = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(graphs, k, layer_sizes, activations1, activations2, dense_layers, dense_units, optimizer, conv, dropout_amount, dropout=None):\n",
    "    generator = PaddedGraphGenerator(graphs=graphs)\n",
    "    k = k  # the number of rows for the output tensor\n",
    "    layer_sizes = layer_sizes\n",
    "\n",
    "    dgcnn_model = DeepGraphCNN(\n",
    "        layer_sizes= layer_sizes,\n",
    "        activations= activations1,\n",
    "        k=k,\n",
    "        bias=False,\n",
    "        generator=generator,\n",
    "    )\n",
    "    x_inp, x_out = dgcnn_model.in_out_tensors()\n",
    "    \n",
    "    if(conv):\n",
    "        x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "    \n",
    "    x_out = Flatten()(x_out)\n",
    "\n",
    "\n",
    "    for i in range(dense_layers):\n",
    "        if((i % dropout_amount) == 0):\n",
    "            if(dropout):\n",
    "                x_out = Dropout(rate=dropout)(x_out)\n",
    "                \n",
    "        if(dense_units < 32):\n",
    "            dense_units = 32\n",
    "    \n",
    "        x_out = Dense(units=dense_units, activation= activations2)(x_out)\n",
    "        dense_units /= 2\n",
    "\n",
    "    \n",
    "    predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "    \n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3928/357997023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'optimizers.Adagrad()'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m model.compile(\n\u001b[0;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary_crossentropy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"acc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3928/793527218.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(graphs, k, layer_sizes, activations1, activations2, dense_layers, dense_units, optimizer, conv, dropout_amount, dropout)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlayer_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     dgcnn_model = DeepGraphCNN(\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mlayer_sizes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mactivations\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mactivations1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'generator'"
     ]
    }
   ],
   "source": [
    "model = create_model(graphs, 25, [16,32], ['tanh', 'tanh'], 'relu', 1, 32, 'optimizers.Adagrad()', 0, 5, None)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(), loss=binary_crossentropy, metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=False, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen, epochs=1000, verbose=1, validation_data=test_gen, shuffle=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce834ef4bb88453c9616f086865b677e44e8db09042ee13be28b090baecb32b6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Ceiec': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
